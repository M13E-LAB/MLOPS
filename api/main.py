#!/usr/bin/env python3
"""
API FastAPI pour la classification d'images
MLOps Project - Pissenlit vs Herbe
"""

import os
import sys
import logging
import uuid
import time
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any
import hashlib
import io

# FastAPI imports
from fastapi import FastAPI, File, UploadFile, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import uvicorn

# ML imports
import torch
from fastai.vision.all import *
from PIL import Image
import numpy as np

# Monitoring imports
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import Response
import psutil

# Database imports
import mysql.connector
from mysql.connector import Error

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration
class Config:
    MODEL_PATH = Path("../model.pkl")
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    ALLOWED_EXTENSIONS = {".jpg", ".jpeg", ".png", ".bmp", ".tiff"}
    IMAGE_SIZE = 224
    
    # Base de donn√©es
    DB_HOST = os.getenv("DB_HOST", "localhost")
    DB_PORT = int(os.getenv("DB_PORT", "3306"))
    DB_NAME = os.getenv("DB_NAME", "mlops_db")
    DB_USER = os.getenv("DB_USER", "mlops_app")
    DB_PASSWORD = os.getenv("DB_PASSWORD", "mlops_app_password")
    
    # API
    API_VERSION = "1.0.0"
    API_TITLE = "MLOps Image Classification API"
    API_DESCRIPTION = "API pour la classification d'images pissenlit vs herbe"

# M√©triques Prometheus
try:
    PREDICTION_COUNTER = Counter('predictions_total', 'Total number of predictions', ['model_version', 'predicted_class'])
    PREDICTION_HISTOGRAM = Histogram('prediction_duration_seconds', 'Time spent on predictions')
    ERROR_COUNTER = Counter('prediction_errors_total', 'Total number of prediction errors', ['error_type'])
    MODEL_INFO = Gauge('model_info', 'Model information', ['model_version', 'model_name'])
except ValueError:
    # M√©triques d√©j√† cr√©√©es, les r√©cup√©rer
    from prometheus_client import REGISTRY
    PREDICTION_COUNTER = None
    PREDICTION_HISTOGRAM = None
    ERROR_COUNTER = None
    MODEL_INFO = None

# Variables globales
model_learner = None
model_version = None
model_loaded_at = None

# Initialisation de l'application FastAPI
app = FastAPI(
    title=Config.API_TITLE,
    description=Config.API_DESCRIPTION,
    version=Config.API_VERSION,
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifier les domaines autoris√©s
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# S√©curit√© (optionnelle)
security = HTTPBearer(auto_error=False)

def get_db_connection():
    """Cr√©er une connexion √† la base de donn√©es"""
    try:
        connection = mysql.connector.connect(
            host=Config.DB_HOST,
            port=Config.DB_PORT,
            database=Config.DB_NAME,
            user=Config.DB_USER,
            password=Config.DB_PASSWORD
        )
        return connection
    except Error as e:
        logger.error(f"Erreur connexion DB: {e}")
        return None

def load_model():
    """Charger le mod√®le de classification"""
    global model_learner, model_version, model_loaded_at
    
    logger.info("üîÑ Chargement du mod√®le...")
    
    if not Config.MODEL_PATH.exists():
        logger.error(f"‚ùå Mod√®le non trouv√©: {Config.MODEL_PATH}")
        return False
    
    try:
        # Charger le mod√®le FastAI
        model_learner = load_learner(Config.MODEL_PATH)
        model_version = f"v{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        model_loaded_at = datetime.now()
        
        # Mettre √† jour les m√©triques
        MODEL_INFO.labels(model_version=model_version, model_name="dandelion-grass-classifier").set(1)
        
        logger.info(f"‚úÖ Mod√®le charg√©: {model_version}")
        logger.info(f"üìä Classes disponibles: {model_learner.dls.vocab}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement mod√®le: {e}")
        return False

def validate_image(file: UploadFile) -> bool:
    """Valider le fichier image upload√©"""
    # V√©rifier l'extension
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in Config.ALLOWED_EXTENSIONS:
        raise HTTPException(
            status_code=400,
            detail=f"Extension non support√©e. Extensions autoris√©es: {Config.ALLOWED_EXTENSIONS}"
        )
    
    # V√©rifier la taille (sera v√©rifi√© lors de la lecture)
    return True

def preprocess_image(image_bytes: bytes) -> Image.Image:
    """Pr√©processer l'image pour la pr√©diction"""
    try:
        # Ouvrir l'image
        image = Image.open(io.BytesIO(image_bytes))
        
        # Convertir en RGB si n√©cessaire
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # V√©rifier les dimensions minimales
        if image.size[0] < 32 or image.size[1] < 32:
            raise HTTPException(status_code=400, detail="Image trop petite (minimum 32x32 pixels)")
        
        return image
        
    except Exception as e:
        logger.error(f"Erreur preprocessing: {e}")
        raise HTTPException(status_code=400, detail=f"Erreur traitement image: {str(e)}")

def log_prediction(request_id: str, prediction_data: Dict[str, Any]):
    """Logger la pr√©diction dans la base de donn√©es"""
    try:
        connection = get_db_connection()
        if connection is None:
            return
        
        cursor = connection.cursor()
        
        query = """
        INSERT INTO api_predictions 
        (request_id, model_name, model_version, predicted_class, confidence_score, 
         prediction_time_ms, response_status, created_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        values = (
            request_id,
            "dandelion-grass-classifier",
            model_version,
            prediction_data.get('predicted_class'),
            prediction_data.get('confidence'),
            prediction_data.get('prediction_time_ms'),
            200,
            datetime.now()
        )
        
        cursor.execute(query, values)
        connection.commit()
        
    except Error as e:
        logger.error(f"Erreur logging DB: {e}")
    finally:
        if connection and connection.is_connected():
            cursor.close()
            connection.close()

# Routes de l'API

@app.on_event("startup")
async def startup_event():
    """√âv√©nement de d√©marrage de l'application"""
    logger.info("üöÄ D√©marrage de l'API MLOps...")
    
    # Charger le mod√®le
    if not load_model():
        logger.error("‚ùå Impossible de charger le mod√®le")
        sys.exit(1)
    
    logger.info("‚úÖ API pr√™te!")

@app.get("/")
async def root():
    """Route racine avec informations sur l'API"""
    return {
        "message": "MLOps Image Classification API",
        "version": Config.API_VERSION,
        "model_version": model_version,
        "model_loaded_at": model_loaded_at.isoformat() if model_loaded_at else None,
        "available_classes": list(model_learner.dls.vocab) if model_learner else [],
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health")
async def health_check():
    """V√©rification de l'√©tat de sant√© de l'API"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": Config.API_VERSION,
        "model_loaded": model_learner is not None,
        "model_version": model_version
    }
    
    # V√©rifier la connexion √† la base de donn√©es
    db_connection = get_db_connection()
    health_status["database_connected"] = db_connection is not None
    if db_connection:
        db_connection.close()
    
    # V√©rifier l'utilisation des ressources
    health_status["system"] = {
        "cpu_percent": psutil.cpu_percent(),
        "memory_percent": psutil.virtual_memory().percent,
        "disk_percent": psutil.disk_usage('/').percent
    }
    
    return health_status

@app.get("/metrics")
async def get_metrics():
    """Endpoint pour les m√©triques Prometheus"""
    return Response(generate_latest(), media_type="text/plain")

@app.post("/predict")
async def predict_image(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """
    Pr√©dire la classe d'une image upload√©e
    
    - **file**: Image √† classifier (formats support√©s: JPG, PNG, BMP, TIFF)
    - Retourne la classe pr√©dite avec le score de confiance
    """
    
    if model_learner is None:
        ERROR_COUNTER.labels(error_type="model_not_loaded").inc()
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    # G√©n√©rer un ID unique pour la requ√™te
    request_id = str(uuid.uuid4())
    start_time = time.time()
    
    try:
        # Valider le fichier
        validate_image(file)
        
        # Lire le contenu du fichier
        image_bytes = await file.read()
        
        # V√©rifier la taille
        if len(image_bytes) > Config.MAX_FILE_SIZE:
            ERROR_COUNTER.labels(error_type="file_too_large").inc()
            raise HTTPException(
                status_code=413, 
                detail=f"Fichier trop volumineux (max: {Config.MAX_FILE_SIZE // (1024*1024)}MB)"
            )
        
        # Pr√©processer l'image
        image = preprocess_image(image_bytes)
        
        # Faire la pr√©diction
        with PREDICTION_HISTOGRAM.time():
            pred_class, pred_idx, pred_probs = model_learner.predict(image)
        
        # Calculer le temps de pr√©diction
        prediction_time_ms = int((time.time() - start_time) * 1000)
        
        # Pr√©parer la r√©ponse
        confidence = float(pred_probs[pred_idx])
        predicted_class = str(pred_class)
        
        response_data = {
            "request_id": request_id,
            "predicted_class": predicted_class,
            "confidence": confidence,
            "all_probabilities": {
                str(class_name): float(prob) 
                for class_name, prob in zip(model_learner.dls.vocab, pred_probs)
            },
            "model_version": model_version,
            "prediction_time_ms": prediction_time_ms,
            "timestamp": datetime.now().isoformat()
        }
        
        # Mettre √† jour les m√©triques
        PREDICTION_COUNTER.labels(
            model_version=model_version, 
            predicted_class=predicted_class
        ).inc()
        
        # Logger la pr√©diction en arri√®re-plan
        background_tasks.add_task(log_prediction, request_id, response_data)
        
        logger.info(f"‚úÖ Pr√©diction {request_id}: {predicted_class} ({confidence:.3f})")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        ERROR_COUNTER.labels(error_type="prediction_error").inc()
        logger.error(f"‚ùå Erreur pr√©diction {request_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur interne: {str(e)}")

@app.post("/predict/batch")
async def predict_batch(
    background_tasks: BackgroundTasks,
    files: list[UploadFile] = File(...),
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """
    Pr√©dire la classe de plusieurs images en lot
    
    - **files**: Liste d'images √† classifier
    - Retourne les pr√©dictions pour chaque image
    """
    
    if model_learner is None:
        ERROR_COUNTER.labels(error_type="model_not_loaded").inc()
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    if len(files) > 10:  # Limiter le nombre d'images par batch
        raise HTTPException(status_code=400, detail="Maximum 10 images par batch")
    
    batch_id = str(uuid.uuid4())
    results = []
    
    for i, file in enumerate(files):
        try:
            # R√©utiliser la logique de pr√©diction simple
            validate_image(file)
            image_bytes = await file.read()
            
            if len(image_bytes) > Config.MAX_FILE_SIZE:
                results.append({
                    "filename": file.filename,
                    "error": "Fichier trop volumineux"
                })
                continue
            
            image = preprocess_image(image_bytes)
            pred_class, pred_idx, pred_probs = model_learner.predict(image)
            
            results.append({
                "filename": file.filename,
                "predicted_class": str(pred_class),
                "confidence": float(pred_probs[pred_idx]),
                "all_probabilities": {
                    str(class_name): float(prob) 
                    for class_name, prob in zip(model_learner.dls.vocab, pred_probs)
                }
            })
            
            PREDICTION_COUNTER.labels(
                model_version=model_version, 
                predicted_class=str(pred_class)
            ).inc()
            
        except Exception as e:
            ERROR_COUNTER.labels(error_type="batch_prediction_error").inc()
            results.append({
                "filename": file.filename,
                "error": str(e)
            })
    
    return {
        "batch_id": batch_id,
        "model_version": model_version,
        "timestamp": datetime.now().isoformat(),
        "results": results
    }

@app.get("/model/info")
async def get_model_info():
    """Obtenir des informations sur le mod√®le actuel"""
    if model_learner is None:
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    return {
        "model_version": model_version,
        "model_loaded_at": model_loaded_at.isoformat(),
        "classes": list(model_learner.dls.vocab),
        "num_classes": len(model_learner.dls.vocab),
        "model_path": str(Config.MODEL_PATH),
        "image_size": Config.IMAGE_SIZE
    }

@app.post("/model/reload")
async def reload_model(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Recharger le mod√®le (utile apr√®s un nouveau d√©ploiement)"""
    logger.info("üîÑ Rechargement du mod√®le demand√©...")
    
    if load_model():
        return {
            "message": "Mod√®le recharg√© avec succ√®s",
            "model_version": model_version,
            "timestamp": datetime.now().isoformat()
        }
    else:
        raise HTTPException(status_code=500, detail="Erreur lors du rechargement du mod√®le")

if __name__ == "__main__":
    # Configuration pour le d√©veloppement
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
