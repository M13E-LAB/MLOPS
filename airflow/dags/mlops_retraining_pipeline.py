"""
DAG MLOps : Pipeline de Retraining Automatique
Auteur: M13E-LAB
Description: Pipeline complet de retraining avec monitoring et d√©ploiement automatique
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.sensors.filesystem import FileSensor
from airflow.utils.dates import days_ago
import pandas as pd
import mlflow
import mlflow.pytorch
import requests
import logging
import os

# Configuration par d√©faut
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False
}

# Configuration MLflow
MLFLOW_TRACKING_URI = "http://mlflow:5000"
MINIO_ENDPOINT = "http://minio:9000"
API_ENDPOINT = "http://mlops-api:8000"

# D√©finition du DAG
dag = DAG(
    'mlops_retraining_pipeline',
    default_args=default_args,
    description='Pipeline MLOps de retraining automatique',
    schedule_interval='@daily',  # Ex√©cution quotidienne
    max_active_runs=1,
    tags=['mlops', 'retraining', 'production']
)

def check_model_performance(**context):
    """
    V√©rifie les performances du mod√®le actuel
    D√©clenche le retraining si les performances sont d√©grad√©es
    """
    logging.info("üîç V√©rification des performances du mod√®le...")
    
    try:
        # Simuler la v√©rification des m√©triques
        # Dans un vrai projet, on r√©cup√©rerait les m√©triques de Prometheus
        response = requests.get(f"{API_ENDPOINT}/metrics", timeout=30)
        
        if response.status_code == 200:
            # Simuler une d√©gradation de performance
            current_accuracy = 0.92  # R√©cup√©rer depuis les m√©triques r√©elles
            threshold_accuracy = 0.95
            
            if current_accuracy < threshold_accuracy:
                logging.warning(f"‚ö†Ô∏è Performance d√©grad√©e: {current_accuracy} < {threshold_accuracy}")
                return "trigger_retraining"
            else:
                logging.info(f"‚úÖ Performance OK: {current_accuracy}")
                return "skip_retraining"
        else:
            logging.error("‚ùå Impossible de r√©cup√©rer les m√©triques")
            return "skip_retraining"
            
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la v√©rification: {str(e)}")
        return "skip_retraining"

def check_new_data(**context):
    """
    V√©rifie s'il y a de nouvelles donn√©es disponibles pour le retraining
    """
    logging.info("üìä V√©rification des nouvelles donn√©es...")
    
    # Simuler la v√©rification de nouvelles donn√©es
    # Dans un vrai projet, on v√©rifierait S3/Minio ou une base de donn√©es
    new_data_available = True  # Simul√© pour la d√©mo
    
    if new_data_available:
        logging.info("‚úÖ Nouvelles donn√©es d√©tect√©es")
        return "proceed_with_retraining"
    else:
        logging.info("‚ÑπÔ∏è Pas de nouvelles donn√©es")
        return "skip_retraining"

def prepare_training_data(**context):
    """
    Pr√©pare les donn√©es pour l'entra√Ænement
    """
    logging.info("üîÑ Pr√©paration des donn√©es d'entra√Ænement...")
    
    try:
        # Simuler la pr√©paration des donn√©es
        # Dans un vrai projet, on t√©l√©chargerait depuis S3/Minio
        
        # Cr√©er un dataset factice pour la d√©mo
        data_info = {
            'total_images': 1000,
            'dandelion_images': 500,
            'grass_images': 500,
            'train_split': 0.8,
            'val_split': 0.2,
            'data_version': datetime.now().strftime("%Y%m%d_%H%M%S")
        }
        
        logging.info(f"üìä Dataset pr√©par√©: {data_info}")
        
        # Stocker les infos dans XCom pour les t√¢ches suivantes
        context['task_instance'].xcom_push(key='data_info', value=data_info)
        
        return "data_prepared_successfully"
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la pr√©paration: {str(e)}")
        raise

def train_new_model(**context):
    """
    Entra√Æne un nouveau mod√®le avec MLflow tracking
    """
    logging.info("ü§ñ D√©marrage de l'entra√Ænement du nouveau mod√®le...")
    
    try:
        # R√©cup√©rer les infos des donn√©es
        data_info = context['task_instance'].xcom_pull(key='data_info')
        
        # Configuration MLflow
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
        experiment_name = "automated_retraining"
        
        try:
            mlflow.create_experiment(experiment_name)
        except:
            pass  # L'exp√©rience existe d√©j√†
            
        mlflow.set_experiment(experiment_name)
        
        with mlflow.start_run(run_name=f"retraining_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
            # Simuler l'entra√Ænement
            # Dans un vrai projet, on utiliserait FastAI ici
            
            # Param√®tres d'entra√Ænement
            params = {
                'learning_rate': 0.001,
                'batch_size': 32,
                'epochs': 10,
                'architecture': 'resnet34',
                'data_version': data_info['data_version']
            }
            
            # Log des param√®tres
            mlflow.log_params(params)
            
            # Simuler les m√©triques d'entra√Ænement
            for epoch in range(params['epochs']):
                train_loss = 0.5 - (epoch * 0.03)  # Simuler une am√©lioration
                val_accuracy = 0.85 + (epoch * 0.01)  # Simuler une am√©lioration
                
                mlflow.log_metrics({
                    'train_loss': train_loss,
                    'val_accuracy': val_accuracy,
                    'epoch': epoch
                }, step=epoch)
            
            # M√©triques finales
            final_metrics = {
                'final_accuracy': 0.96,
                'final_loss': 0.15,
                'f1_score': 0.95,
                'precision': 0.94,
                'recall': 0.97
            }
            
            mlflow.log_metrics(final_metrics)
            
            # Simuler la sauvegarde du mod√®le
            model_path = f"models/model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            
            # Dans un vrai projet, on sauvegarderait le vrai mod√®le
            mlflow.log_artifact(__file__, "model_code")  # Log du code pour la d√©mo
            
            # Enregistrer le mod√®le dans le Model Registry
            model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
            model_version = mlflow.register_model(
                model_uri=model_uri,
                name="dandelion_grass_classifier"
            )
            
            logging.info(f"‚úÖ Mod√®le entra√Æn√© avec succ√®s!")
            logging.info(f"üìä Accuracy finale: {final_metrics['final_accuracy']}")
            logging.info(f"üè∑Ô∏è Version du mod√®le: {model_version.version}")
            
            # Stocker les infos du mod√®le pour la validation
            model_info = {
                'run_id': mlflow.active_run().info.run_id,
                'model_version': model_version.version,
                'accuracy': final_metrics['final_accuracy'],
                'model_path': model_path
            }
            
            context['task_instance'].xcom_push(key='model_info', value=model_info)
            
            return "model_trained_successfully"
            
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de l'entra√Ænement: {str(e)}")
        raise

def validate_new_model(**context):
    """
    Valide le nouveau mod√®le avant d√©ploiement
    """
    logging.info("üß™ Validation du nouveau mod√®le...")
    
    try:
        model_info = context['task_instance'].xcom_pull(key='model_info')
        
        # Crit√®res de validation
        min_accuracy = 0.93
        current_accuracy = model_info['accuracy']
        
        if current_accuracy >= min_accuracy:
            logging.info(f"‚úÖ Mod√®le valid√©: {current_accuracy} >= {min_accuracy}")
            return "model_validated"
        else:
            logging.warning(f"‚ùå Mod√®le rejet√©: {current_accuracy} < {min_accuracy}")
            return "model_rejected"
            
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la validation: {str(e)}")
        raise

def deploy_new_model(**context):
    """
    D√©ploie le nouveau mod√®le en production
    """
    logging.info("üöÄ D√©ploiement du nouveau mod√®le...")
    
    try:
        model_info = context['task_instance'].xcom_pull(key='model_info')
        
        # Dans un vrai projet, on mettrait √† jour l'API avec le nouveau mod√®le
        # Ici on simule le d√©ploiement
        
        # Transition du mod√®le vers "Production" dans MLflow
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
        client = mlflow.tracking.MlflowClient()
        
        client.transition_model_version_stage(
            name="dandelion_grass_classifier",
            version=model_info['model_version'],
            stage="Production"
        )
        
        logging.info(f"‚úÖ Mod√®le {model_info['model_version']} d√©ploy√© en production!")
        
        # Notifier l'API du nouveau mod√®le (simulation)
        try:
            response = requests.post(
                f"{API_ENDPOINT}/reload_model",
                json={'model_version': model_info['model_version']},
                timeout=30
            )
            if response.status_code == 200:
                logging.info("‚úÖ API mise √† jour avec le nouveau mod√®le")
            else:
                logging.warning("‚ö†Ô∏è Impossible de notifier l'API")
        except:
            logging.warning("‚ö†Ô∏è API non disponible pour la notification")
        
        return "deployment_successful"
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors du d√©ploiement: {str(e)}")
        raise

def send_notification(**context):
    """
    Envoie une notification de fin de pipeline
    """
    logging.info("üìß Envoi de la notification...")
    
    try:
        model_info = context['task_instance'].xcom_pull(key='model_info')
        
        # Dans un vrai projet, on enverrait un email/Slack
        notification = {
            'pipeline': 'mlops_retraining_pipeline',
            'status': 'SUCCESS',
            'model_version': model_info.get('model_version', 'N/A'),
            'accuracy': model_info.get('accuracy', 'N/A'),
            'timestamp': datetime.now().isoformat()
        }
        
        logging.info(f"üìß Notification: {notification}")
        
        return "notification_sent"
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la notification: {str(e)}")
        # Ne pas faire √©chouer le pipeline pour une notification
        return "notification_failed"

# D√©finition des t√¢ches
start_task = DummyOperator(
    task_id='start_pipeline',
    dag=dag
)

check_performance_task = PythonOperator(
    task_id='check_model_performance',
    python_callable=check_model_performance,
    dag=dag
)

check_data_task = PythonOperator(
    task_id='check_new_data',
    python_callable=check_new_data,
    dag=dag
)

prepare_data_task = PythonOperator(
    task_id='prepare_training_data',
    python_callable=prepare_training_data,
    dag=dag
)

train_model_task = PythonOperator(
    task_id='train_new_model',
    python_callable=train_new_model,
    dag=dag
)

validate_model_task = PythonOperator(
    task_id='validate_new_model',
    python_callable=validate_new_model,
    dag=dag
)

deploy_model_task = PythonOperator(
    task_id='deploy_new_model',
    python_callable=deploy_new_model,
    dag=dag
)

notification_task = PythonOperator(
    task_id='send_notification',
    python_callable=send_notification,
    dag=dag
)

skip_retraining_task = DummyOperator(
    task_id='skip_retraining',
    dag=dag
)

end_task = DummyOperator(
    task_id='end_pipeline',
    dag=dag
)

# D√©finition des d√©pendances
start_task >> [check_performance_task, check_data_task]

# Logique conditionnelle (simplifi√©e pour la d√©mo)
check_performance_task >> prepare_data_task
check_data_task >> prepare_data_task

prepare_data_task >> train_model_task
train_model_task >> validate_model_task
validate_model_task >> deploy_model_task
deploy_model_task >> notification_task

# T√¢ches de fin
notification_task >> end_task
skip_retraining_task >> end_task
