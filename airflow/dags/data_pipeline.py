"""
DAG Airflow pour l'extraction et le preprocessing des donn√©es
Pipeline MLOps - Classification Pissenlit vs Herbe
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.mysql.hooks.mysql import MySqlHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import requests
import pandas as pd
from PIL import Image
import io
import hashlib
import logging
import os
from pathlib import Path

# Configuration par d√©faut du DAG
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 11, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# D√©finition du DAG
dag = DAG(
    'data_extraction_pipeline',
    default_args=default_args,
    description='Pipeline d\'extraction et preprocessing des donn√©es d\'images',
    schedule_interval='@daily',  # Ex√©cution quotidienne
    catchup=False,
    max_active_runs=1,
    tags=['mlops', 'data', 'preprocessing'],
)

def extract_data_from_db(**context):
    """Extraire les m√©tadonn√©es des images depuis la base de donn√©es"""
    logging.info("üîç Extraction des donn√©es depuis la base de donn√©es...")
    
    # Connexion √† MySQL
    mysql_hook = MySqlHook(mysql_conn_id='mysql_default')
    
    # Requ√™te pour r√©cup√©rer les images non t√©l√©charg√©es
    query = """
    SELECT id, url_source, label, filename 
    FROM plants_data 
    WHERE download_status = 'pending' 
    ORDER BY id 
    LIMIT 50
    """
    
    df = mysql_hook.get_pandas_df(query)
    logging.info(f"üìä {len(df)} images √† traiter")
    
    # Sauvegarder dans XCom pour les t√¢ches suivantes
    return df.to_json(orient='records')

def download_and_process_images(**context):
    """T√©l√©charger et traiter les images"""
    logging.info("üì• T√©l√©chargement et traitement des images...")
    
    # R√©cup√©rer les donn√©es depuis XCom
    data_json = context['task_instance'].xcom_pull(task_ids='extract_data')
    if not data_json:
        logging.info("Aucune donn√©e √† traiter")
        return
    
    import json
    data = json.loads(data_json)
    
    # Configuration S3/Minio
    s3_hook = S3Hook(aws_conn_id='minio_default')
    bucket_name = 'mlops-images'
    
    # Cr√©er le bucket s'il n'existe pas
    if not s3_hook.check_for_bucket(bucket_name):
        s3_hook.create_bucket(bucket_name)
        logging.info(f"‚úÖ Bucket {bucket_name} cr√©√©")
    
    mysql_hook = MySqlHook(mysql_conn_id='mysql_default')
    processed_count = 0
    failed_count = 0
    
    for item in data:
        try:
            # T√©l√©charger l'image
            response = requests.get(item['url_source'], timeout=30)
            response.raise_for_status()
            
            # V√©rifier que c'est bien une image
            image = Image.open(io.BytesIO(response.content))
            width, height = image.size
            file_size = len(response.content)
            
            # G√©n√©rer le hash du fichier
            file_hash = hashlib.md5(response.content).hexdigest()
            
            # Chemin S3
            s3_key = f"{item['label']}/{item['filename']}"
            
            # Uploader vers S3/Minio
            s3_hook.load_bytes(
                bytes_data=response.content,
                key=s3_key,
                bucket_name=bucket_name,
                replace=True
            )
            
            # URL S3
            s3_url = f"s3://{bucket_name}/{s3_key}"
            
            # Mettre √† jour la base de donn√©es
            update_query = """
            UPDATE plants_data 
            SET url_s3 = %s, 
                file_size = %s, 
                image_width = %s, 
                image_height = %s,
                download_status = 'downloaded',
                updated_at = NOW()
            WHERE id = %s
            """
            
            mysql_hook.run(update_query, parameters=[
                s3_url, file_size, width, height, item['id']
            ])
            
            processed_count += 1
            logging.info(f"‚úÖ {item['filename']} trait√© avec succ√®s")
            
        except Exception as e:
            logging.error(f"‚ùå Erreur avec {item['filename']}: {str(e)}")
            
            # Marquer comme √©chou√© dans la DB
            error_query = """
            UPDATE plants_data 
            SET download_status = 'failed',
                updated_at = NOW()
            WHERE id = %s
            """
            mysql_hook.run(error_query, parameters=[item['id']])
            failed_count += 1
    
    logging.info(f"üìä R√©sum√©: {processed_count} succ√®s, {failed_count} √©checs")
    return {'processed': processed_count, 'failed': failed_count}

def validate_data_quality(**context):
    """Valider la qualit√© des donn√©es t√©l√©charg√©es"""
    logging.info("üîç Validation de la qualit√© des donn√©es...")
    
    mysql_hook = MySqlHook(mysql_conn_id='mysql_default')
    
    # Statistiques g√©n√©rales
    stats_query = """
    SELECT 
        label,
        download_status,
        COUNT(*) as count,
        AVG(file_size) as avg_file_size,
        AVG(image_width) as avg_width,
        AVG(image_height) as avg_height
    FROM plants_data 
    GROUP BY label, download_status
    """
    
    df_stats = mysql_hook.get_pandas_df(stats_query)
    logging.info("üìä Statistiques par classe et statut:")
    logging.info(df_stats.to_string())
    
    # V√©rifications de qualit√©
    quality_checks = []
    
    # Check 1: Images trop petites
    small_images_query = """
    SELECT COUNT(*) as count 
    FROM plants_data 
    WHERE download_status = 'downloaded' 
    AND (image_width < 50 OR image_height < 50)
    """
    small_count = mysql_hook.get_first(small_images_query)[0]
    quality_checks.append(f"Images trop petites: {small_count}")
    
    # Check 2: √âquilibre des classes
    balance_query = """
    SELECT label, COUNT(*) as count 
    FROM plants_data 
    WHERE download_status = 'downloaded' 
    GROUP BY label
    """
    balance_df = mysql_hook.get_pandas_df(balance_query)
    if len(balance_df) == 2:
        ratio = balance_df['count'].max() / balance_df['count'].min()
        quality_checks.append(f"Ratio d√©s√©quilibre classes: {ratio:.2f}")
    
    # Check 3: Taux de succ√®s du t√©l√©chargement
    success_query = """
    SELECT 
        download_status,
        COUNT(*) as count,
        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM plants_data), 2) as percentage
    FROM plants_data 
    GROUP BY download_status
    """
    success_df = mysql_hook.get_pandas_df(success_query)
    logging.info("üìà Taux de succ√®s du t√©l√©chargement:")
    logging.info(success_df.to_string())
    
    # Retourner les r√©sultats
    return {
        'quality_checks': quality_checks,
        'stats': df_stats.to_dict('records'),
        'success_rates': success_df.to_dict('records')
    }

def prepare_training_data(**context):
    """Pr√©parer les donn√©es pour l'entra√Ænement"""
    logging.info("üéØ Pr√©paration des donn√©es d'entra√Ænement...")
    
    mysql_hook = MySqlHook(mysql_conn_id='mysql_default')
    
    # R√©cup√©rer toutes les images t√©l√©charg√©es avec succ√®s
    query = """
    SELECT id, url_s3, label, filename, image_width, image_height, file_size
    FROM plants_data 
    WHERE download_status = 'downloaded'
    ORDER BY label, id
    """
    
    df = mysql_hook.get_pandas_df(query)
    logging.info(f"üìä {len(df)} images disponibles pour l'entra√Ænement")
    
    # Statistiques par classe
    class_stats = df.groupby('label').agg({
        'id': 'count',
        'file_size': ['mean', 'std'],
        'image_width': ['mean', 'std'],
        'image_height': ['mean', 'std']
    }).round(2)
    
    logging.info("üìà Statistiques par classe:")
    logging.info(class_stats.to_string())
    
    # Cr√©er les splits train/validation/test (70/15/15)
    train_data = []
    val_data = []
    test_data = []
    
    for label in df['label'].unique():
        label_data = df[df['label'] == label].sample(frac=1, random_state=42)  # M√©langer
        n = len(label_data)
        
        train_end = int(0.7 * n)
        val_end = int(0.85 * n)
        
        train_data.extend(label_data.iloc[:train_end].to_dict('records'))
        val_data.extend(label_data.iloc[train_end:val_end].to_dict('records'))
        test_data.extend(label_data.iloc[val_end:].to_dict('records'))
    
    logging.info(f"üìä Splits cr√©√©s: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")
    
    # Sauvegarder les m√©tadonn√©es des splits
    splits_data = {
        'train': train_data,
        'validation': val_data,
        'test': test_data,
        'stats': class_stats.to_dict(),
        'total_images': len(df)
    }
    
    return splits_data

def trigger_model_training(**context):
    """D√©clencher l'entra√Ænement du mod√®le si les donn√©es sont pr√™tes"""
    logging.info("üöÄ V√©rification si l'entra√Ænement peut √™tre d√©clench√©...")
    
    # R√©cup√©rer les donn√©es des splits
    splits_data = context['task_instance'].xcom_pull(task_ids='prepare_training_data')
    
    if not splits_data:
        logging.warning("Aucune donn√©e d'entra√Ænement disponible")
        return False
    
    train_size = len(splits_data['train'])
    val_size = len(splits_data['validation'])
    
    # Crit√®res pour d√©clencher l'entra√Ænement
    min_train_size = 100  # Minimum 100 images d'entra√Ænement
    min_val_size = 20     # Minimum 20 images de validation
    
    if train_size >= min_train_size and val_size >= min_val_size:
        logging.info(f"‚úÖ Crit√®res remplis: Train={train_size}, Val={val_size}")
        logging.info("üéØ D√©clenchement de l'entra√Ænement du mod√®le...")
        
        # Ici, on pourrait d√©clencher un autre DAG pour l'entra√Ænement
        # ou envoyer un signal √† un service externe
        
        return True
    else:
        logging.warning(f"‚ùå Crit√®res non remplis: Train={train_size}, Val={val_size}")
        return False

# D√©finition des t√¢ches
extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data_from_db,
    dag=dag,
)

download_task = PythonOperator(
    task_id='download_and_process_images',
    python_callable=download_and_process_images,
    dag=dag,
)

validate_task = PythonOperator(
    task_id='validate_data_quality',
    python_callable=validate_data_quality,
    dag=dag,
)

prepare_task = PythonOperator(
    task_id='prepare_training_data',
    python_callable=prepare_training_data,
    dag=dag,
)

trigger_task = PythonOperator(
    task_id='trigger_model_training',
    python_callable=trigger_model_training,
    dag=dag,
)

# T√¢che de nettoyage (optionnelle)
cleanup_task = BashOperator(
    task_id='cleanup_temp_files',
    bash_command='echo "üßπ Nettoyage des fichiers temporaires termin√©"',
    dag=dag,
)

# D√©finition des d√©pendances
extract_task >> download_task >> validate_task >> prepare_task >> trigger_task >> cleanup_task
